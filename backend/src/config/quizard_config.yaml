# config/quizard_config.yaml
model:
  model_name: "gpt-4o-mini"
  temperature: 0.8
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  # For max_tokens see completion_limit

# Token limits are to be tweaked and optimized during development, but left alone during production
token_limits:
  # TODO: Optimize...gpt-4o-mini has a context window of 128,000 tokens
  # APP PARAM: Token limit for the application to use (must be <= OpenAI's token limit for the model used). Note: on must use less than the max.
#  possible context length since tiktoken is inaccurate.
#  4K proved to generate more flashcards
  app_limit: 3800
  # APP PARAM: Token limit for the completion generated by the model.
  completion_limit: 800
  # DEV PARAM: Token limit for the fixed app prompts (i.e. system prompts + additional prompts + examples).
  prompt_limit: 2000


# It follows that the token limit for the variable input text is app_limit - (prompt_limit + completion_limit)
# For input text that exceed this limit text splitting is used.
# The length of each text fragment (window size) generated by text splitting will be equal to (or smaller than) than the input token limit.
# The overlap between the text fragments can be either set relative to the text fragment being processed via relative_window_overlap or set to an
# absolute token value.
text_splitting:
  overlap_type: "absolute"    # "relative" or "absolute"
  overlap: 100  # if "relative" expect values in range [0, 1), if "absolute" expect number of characters as measure of overlap

  # Optimal parameters:
  # 3.000, 2.400 and 400 when using the 4k model
  # _____, 10.000 and 10.400 when using the 16k model

prompts:
  example_prompt: "ex1_genetics_of_cancer"     # Name of the example directory
  additional_prompt: "ad1_top_instr"           # Prompt to be inserted at the top of the input text