# config/quizard_config.yaml
# TODO: Optimize and test parameters
# 07.02.2024 Model overview:
# gpt-3.5-turbo-instruct:       context:    4K;   input: $0.0015 / 1K tokens;   output: $0.0020 / 1K tokens
# gpt-3.5-turbo-0125:           context:   16K;   input: $0.0005 / 1K tokens;   output: $0.0015 / 1K tokens
# gpt-4-0125-preview	        context:  128K;   input: $0.0005 / 1K tokens;   output: $0.0015 / 1K tokens
# gpt-4-0125-vision-preview	    context:  128K;   input: $0.0005 / 1K tokens;   output: $0.0015 / 1K tokens (additional overhead for vision)
# gpt-4 	                    context:   16K;   input: $0.0300 / 1K tokens;   output: $0.0600 / 1K tokens
# gpt-4-32k         	        context:   32K;   input: $0.0600 / 1K tokens;   output: $0.1200 / 1K tokens
model:
  model_name: "gpt-3.5-turbo"
  temperature: 0.8
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  # For max_tokens see completion_limit

# Token limits are to be tweaked and optimized during development, but left alone during production
token_limits:
  # APP PARAM: Token limit for the application to use (must be <= OpenAI's token limit for the model used). Note: on must use less than the max.
#  possible context length since tiktoken is inaccurate.
#  4K proved to generate more flashcards
  app_limit: 3800
  # APP PARAM: Token limit for the completion generated by the model.
  completion_limit: 800
  # DEV PARAM: Token limit for the fixed app prompts (i.e. system prompts + additional prompts + examples).
  prompt_limit: 2000


# It follows that the token limit for the variable input text is app_limit - (prompt_limit + completion_limit)
# For input text that exceed this limit text splitting is used.
# The length of each text fragment (window size) generated by text splitting will be equal to (or smaller than) than the input token limit.
# The overlap between the text fragments can be either set relative to the text fragment being processed via relative_window_overlap or set to an
# absolute token value.
text_splitting:
  overlap_type: "absolute"    # "relative" or "absolute"
  overlap: 100  # if "relative" expect values in range [0, 1), if "absolute" expect number of characters as measure of overlap

  # Optimal parameters:
  # 3.000, 2.400 and 400 when using the 4k model
  # _____, 10.000 and 10.400 when using the 16k model

prompts:
  example_prompt: "ex1_genetics_of_cancer"     # Name of the example directory
  additional_prompt: "ad1_top_instr"           # Prompt to be inserted at the top of the input text